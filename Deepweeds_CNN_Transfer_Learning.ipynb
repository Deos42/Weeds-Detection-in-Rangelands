{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ojV0Oe5BMEOG",
    "outputId": "01d368fa-9cd7-4050-97a4-7a310373af7d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. SETUP & IMPORTS\n",
    "# ==========================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from google.colab import files\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import tensorflow_datasets as tfds\n",
    "from PIL import Image\n",
    "\n",
    "# Device configuration (Use GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ==========================================\n",
    "# 2. DATA DOWNLOAD & CONVERSION\n",
    "# ==========================================\n",
    "# I use TFDS to download because it's Google's most stable mirror for Colab\n",
    "print(\"Downloading DeepWeeds via stable mirror...\")\n",
    "ds_builder = tfds.builder(\"deep_weeds\")\n",
    "ds_builder.download_and_prepare()\n",
    "\n",
    "# Create organized folders for PyTorch\n",
    "base_dir = \"deepweeds_pytorch\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "print(\"Converting dataset to PyTorch-friendly format...\")\n",
    "info = ds_builder.info\n",
    "class_names = info.features[\"label\"].names\n",
    "\n",
    "# Extract images from the downloaded archives into class folders\n",
    "for split in [\"train\"]:\n",
    "    ds = tfds.as_numpy(ds_builder.as_dataset(split=split))\n",
    "    for i, example in enumerate(ds):\n",
    "        image = Image.fromarray(example[\"image\"])\n",
    "        label_idx = example[\"label\"]\n",
    "        label_name = class_names[label_idx]\n",
    "\n",
    "        class_folder = os.path.join(base_dir, label_name)\n",
    "        os.makedirs(class_folder, exist_ok=True)\n",
    "\n",
    "        image.save(os.path.join(class_folder, f\"img_{i}.jpg\"))\n",
    "        if i % 1000 == 0: print(f\"Processed {i} images...\")\n",
    "\n",
    "print(\"✅ Dataset successfully organized for PyTorch!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544,
     "referenced_widgets": [
      "d0b4a302533f48b485da4258e8d23f35",
      "49a2248943854cb8a94048e9978d0f80",
      "70f92adae7134e10b1bdce4ee8719334",
      "2341d0c0b40d4f1b85dabd65492abfff",
      "2438f00f1d964ea4aeafadc9f101aeb2",
      "df77415efa654dd685e3cbec14c006e8",
      "77295a21f80d4f3080d154695e3ae8b1",
      "2c659305d2b54684ad15cae24348f908",
      "84dc09e9e1194fc698b6be363f0cba87",
      "1e83aab5c2ef4418acc7c302c5de010d",
      "3b708e37cce94103a95ad30c1fd25637",
      "06fc9c60d8004d6cb5997fba41d4d9dc",
      "2ab9a67e8d704374bac4c4377feb9dfe",
      "21d388757db8482d9fe5644cede60345",
      "b808381f44544e7c8ea85d4215b3e3cb",
      "8798c7877d6f4ada82bc7d53debb3b5c",
      "cec299ba3d594f1a88f8ac4fcc4525ad",
      "ae48fe494a4241339efc28f1640b4051",
      "28dc3b8684804f098ed04b167ee9142e",
      "076c7719c0f840da8f1774dfccda6313",
      "f7b556b2e8a74d2e894229b73b491482",
      "6b454b55422b45fa9f5aa781ac8d0990",
      "f3da02f3e59d4ac6a91c64ed1ddf1123",
      "cb05f6ca6c1648e9a5be4a33f65da11a",
      "202547717a4c433ea2aefeffffe23b29",
      "0b7d9a643f154f34a98dc58f5578c1a4",
      "cc1237dab81743cfa49301e6a696604c",
      "b433d4b355a14f9c8f4dda22ceb57304",
      "be52d694951a4b63a547fcddb107e9ee",
      "d3c376dd40894b81a932c7e396eb2d79",
      "5ff8d9ef20c74428bfe3681e04b09fb6",
      "a8db50d81a1b4abd89ed24915248784c",
      "238ba99bdb2f4a239fa662dc0ff5897b",
      "63e62642a640463da30f9955af300bce",
      "f6be62a8ae124f2cad5a5ad964c291bc",
      "a330e43c0b3c4f8ea62a0c6fb9bddc5a",
      "747cbc75a0ca4bb2b53f2dc15514a719",
      "e0fdd20df7b94da0ae03a67a2efcfeff",
      "ac8070828c914e10ba1f213dac60f92f",
      "fb251d444e274161837e1670aeb8a3a9",
      "6b6cddf350a44a7bb008f3ce4bc28a52",
      "fecf2882ea764811be1016ba13bd2367",
      "b1cb0def406b44d9835914e849850f02",
      "45df12f77b3645ca9cc44559abe9912b",
      "2ab80262ffd44a83933ee1d7eb8cabf9",
      "83963e4dc99b4fdf9e8dae20372549f9",
      "c31e5fb2888146ceaef43e79a0f07b0b",
      "f4bea457a56a4b6a817a20c98c68f4fd",
      "39ad7c61be604ced958d3ad92dc83a8b",
      "eb1fd5a35ffd47dc9b523e31e0a6e19a",
      "4b3c9944051c438e989f90b04a7b495f",
      "20b53c45a48247fe877587a947a62882",
      "466dfc323ede4e279211783b82764189",
      "c3a707e964f44da5b97e2e0b00322f10",
      "e6b9ff4b7cc540cbae4316e971c0d774",
      "a9b588ea34874c0387d6a7ec07cb7b40",
      "d6698b19c5ff438f9db297088bb35812",
      "7480f754fc1947ffa39e47299bd2a76a",
      "f4804179279e4284acd628f77dd064a9",
      "d1cf7738a9ef446fb212a839e52d5c75",
      "c687a9d59f1f456f8f49795092bc1f6d",
      "edfa6cd188704e4f87632cf0f179a329",
      "d83b8d58e6f948d4b99d235045aef1d5",
      "8961da3c645e4deea70e846d2021acf4",
      "c015f159e7cf492099f520330a087a74",
      "cad6e1d034b44e37803bd31fc466983d"
     ]
    },
    "id": "wMAaxrZbMUJ3",
    "outputId": "9cfc5f06-4156-4741-9f80-e134ebf954fb"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading DeepWeeds via stable mirror...\n",
      "Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/deep_weeds/3.0.0...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0b4a302533f48b485da4258e8d23f35"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06fc9c60d8004d6cb5997fba41d4d9dc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f3da02f3e59d4ac6a91c64ed1ddf1123"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "63e62642a640463da30f9955af300bce"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2ab80262ffd44a83933ee1d7eb8cabf9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Shuffling /root/tensorflow_datasets/deep_weeds/incomplete.O8C48O_3.0.0/deep_weeds-train.tfrecord*...:   0%|   …"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a9b588ea34874c0387d6a7ec07cb7b40"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset deep_weeds downloaded and prepared to /root/tensorflow_datasets/deep_weeds/3.0.0. Subsequent calls will reuse this data.\n",
      "Converting dataset to PyTorch-friendly format...\n",
      "Processed 0 images...\n",
      "Processed 1000 images...\n",
      "Processed 2000 images...\n",
      "Processed 3000 images...\n",
      "Processed 4000 images...\n",
      "Processed 5000 images...\n",
      "Processed 6000 images...\n",
      "Processed 7000 images...\n",
      "Processed 8000 images...\n",
      "Processed 9000 images...\n",
      "Processed 10000 images...\n",
      "Processed 11000 images...\n",
      "Processed 12000 images...\n",
      "Processed 13000 images...\n",
      "Processed 14000 images...\n",
      "Processed 15000 images...\n",
      "Processed 16000 images...\n",
      "Processed 17000 images...\n",
      "✅ Dataset successfully organized for PyTorch!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ==========================================\n",
    "# 3. DATA PREPROCESSING\n",
    "# ==========================================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "full_dataset = datasets.ImageFolder(base_dir, transform=data_transforms['train'])\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = data_transforms['val']\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_dataset, batch_size=32, shuffle=True),\n",
    "    'val': DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "}\n"
   ],
   "metadata": {
    "id": "HzkMPpZ2Macb"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ==========================================\n",
    "# 4. MODEL: RESNET50 (Partial Fine Tuning)\n",
    "# ==========================================\n",
    "model = models.resnet50(weights='IMAGENET1K_V1')\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace final layer for the 9 DeepWeeds classes\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BZHSsyehNG9a",
    "outputId": "419e7e1c-dd03-4e70-f398-882ee9f33f90"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 135MB/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ==========================================\n",
    "# 5. TRAINING & METRICS\n",
    "# ==========================================\n",
    "def train(epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in dataloaders['train']:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloaders['val']:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Loss: {running_loss/len(dataloaders['train']):.4f} | Val Acc: {correct.double()/val_size:.4f}\")\n",
    "\n",
    "train(epochs=5)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EyNFHAAaNKK_",
    "outputId": "a1d6edee-7cae-4f0d-f578-e2b8f88e95cd"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 | Loss: 0.9684 | Val Acc: 0.7407\n",
      "Epoch 2 | Loss: 0.7233 | Val Acc: 0.7641\n",
      "Epoch 3 | Loss: 0.6663 | Val Acc: 0.7730\n",
      "Epoch 4 | Loss: 0.6266 | Val Acc: 0.7895\n",
      "Epoch 5 | Loss: 0.6087 | Val Acc: 0.7964\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def evaluate_model_metrics(dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    print(\"Calculating final metrics on validation set...\")\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate individual metrics\n",
    "    # 'weighted' accounts for label imbalance in the dataset\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"Test Set Performance:\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Accuracy  : {accuracy:.4f}\")\n",
    "    print(f\"Precision : {precision:.4f}\")\n",
    "    print(f\"Recall    : {recall:.4f}\")\n",
    "    print(f\"F1-score  : {f1:.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    return accuracy, precision, recall, f1, cm\n",
    "\n",
    "\n",
    "metrics = evaluate_model_metrics(dataloaders['val'])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uLPmU6PBNNWg",
    "outputId": "4af70984-adcb-42d8-e814-3e4d35e145ba"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Calculating final metrics on validation set...\n",
      "\n",
      "========================================\n",
      "Test Set Performance:\n",
      "========================================\n",
      "Accuracy  : 0.7964\n",
      "Precision : 0.8043\n",
      "Recall    : 0.7964\n",
      "F1-score  : 0.7963\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 128    5   35    1   14    1    3    0   26]\n",
      " [   8  158   52    0    5    0    9    2   12]\n",
      " [  29   17 1599   12   61   23   29   14   33]\n",
      " [   0    0   19  175    7    5    0    0    0]\n",
      " [   1    0   28    3  162    3    2    0    0]\n",
      " [   0    0   22    6   19  144    0    0    1]\n",
      " [   2    1   40    3   10    0  158    1    5]\n",
      " [   1    6   64    1    2    0    4  151    3]\n",
      " [  19    1   31    0   10    0    1    1  114]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "m1z7OY79PB9m"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
